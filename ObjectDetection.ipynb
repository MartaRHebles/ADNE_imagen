{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ddcf13a",
   "metadata": {},
   "source": [
    "# YOLO Object Detection\n",
    "\n",
    "- YOLO takes a completely different approach.\n",
    "- Itâ€™s not a traditional classifier that is repurposed to be an object detector.\n",
    "- YOLO actually looks at the image just once (hence its name: You Only Look Once) but in a clever way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3a226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/marta/Desktop/datos no estuct/ADNE_imagen')\n",
    "\n",
    "from utils import *\n",
    "from darknet import Darknet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68327b22",
   "metadata": {},
   "source": [
    "*Setting Up The Neural Network*\n",
    "\n",
    "We will be using the latest version of YOLO, known as YOLOv3. We have already downloaded the `yolov3.cfg` file that contains the network architecture used by YOLOv3 and placed it in the `/cfg/` folder. Similarly, we have placed the `yolov3.weights` file that contains the pre-trained weights in the `/weights/` directory. Finally, the `melanomas.names` file that has the list of the 80 object classes that the weights were trained to detect.\n",
    "\n",
    "In the code below, we start by specifying the location of the files that contain the neural network architecture, the pre-trained weights, and the object classes.  We then use *Darknet* to setup the neural network using the network architecture specified in the `cfg_file`. We then use the`.load_weights()` method to load our set of pre-trained weights into the model. Finally, we use the `load_class_names()` function, from the `utils` module, to load the 80 object classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b15633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading weights. Please Wait...100.00% Complete\r"
     ]
    }
   ],
   "source": [
    "# Set the location and name of the cfg file\n",
    "cfg_file = './cfg/yolov3.cfg'\n",
    "\n",
    "# Set the location and name of the pre-trained weights file\n",
    "weight_file = './weights/yolov3.weights'\n",
    "\n",
    "# Set the location and name of the object classes file\n",
    "namesfile = 'melanomas.names'\n",
    "\n",
    "# Load the network architecture\n",
    "m = Darknet(cfg_file)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "m.load_weights(weight_file)\n",
    "\n",
    "# Load the COCO object classes\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde5d04",
   "metadata": {},
   "source": [
    "*Taking a Look at The Neural Network*\n",
    "\n",
    "Now that the neural network has been setup, we can see what it looks like. We can print the network using the `.print_network()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e3cc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer     filters    size              input                output\n",
      "    0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32\n",
      "    1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64\n",
      "    2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32\n",
      "    3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64\n",
      "    4 shortcut 1\n",
      "    5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128\n",
      "    6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64\n",
      "    7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    8 shortcut 5\n",
      "    9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64\n",
      "   10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "   11 shortcut 8\n",
      "   12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256\n",
      "   13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   15 shortcut 12\n",
      "   16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   18 shortcut 15\n",
      "   19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   21 shortcut 18\n",
      "   22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   24 shortcut 21\n",
      "   25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   27 shortcut 24\n",
      "   28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   30 shortcut 27\n",
      "   31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   33 shortcut 30\n",
      "   34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   36 shortcut 33\n",
      "   37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512\n",
      "   38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   40 shortcut 37\n",
      "   41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   43 shortcut 40\n",
      "   44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   46 shortcut 43\n",
      "   47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   49 shortcut 46\n",
      "   50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   52 shortcut 49\n",
      "   53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   55 shortcut 52\n",
      "   56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   58 shortcut 55\n",
      "   59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   61 shortcut 58\n",
      "   62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024\n",
      "   63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   65 shortcut 62\n",
      "   66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   68 shortcut 65\n",
      "   69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   71 shortcut 68\n",
      "   72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   74 shortcut 71\n",
      "   75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   81 conv    255  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 255\n",
      "   82 detection\n",
      "   83 route  79\n",
      "   84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256\n",
      "   85 upsample           * 2    13 x  13 x 256   ->    26 x  26 x 256\n",
      "   86 route  85 61\n",
      "   87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256\n",
      "   88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   93 conv    255  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 255\n",
      "   94 detection\n",
      "   95 route  91\n",
      "   96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128\n",
      "   97 upsample           * 2    26 x  26 x 128   ->    52 x  52 x 128\n",
      "   98 route  97 36\n",
      "   99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128\n",
      "  100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "  101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "  102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "  103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "  104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "  105 conv    255  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 255\n",
      "  106 detection\n"
     ]
    }
   ],
   "source": [
    "# Print the neural network used in YOLOv3\n",
    "m.print_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e59a26",
   "metadata": {},
   "source": [
    "As we can see, the neural network used by YOLOv3 consists mainly of convolutional layers, with some shortcut connections and upsample layers. For a full description of this network please refer to the <a href=\"https://pjreddie.com/media/files/papers/YOLOv3.pdf\">YOLOv3 Paper</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2106e17",
   "metadata": {},
   "source": [
    "*Loading and Resizing Our Images*\n",
    "\n",
    "In the code below, we load our images using OpenCV's `cv2.imread()` function. Since, this function loads images as BGR we will convert our images to RGB so we can display them with the correct colors.\n",
    "\n",
    "As we can see in the previous cell, the input size of the first layer of the network is 416 x 416 x 3. Since images have different sizes, we have to resize our images to be compatible with the input size of the first layer in the network. In the code below, we resize our images using OpenCV's `cv2.resize()` function. We then plot the original and resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc3816",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread('Melanoma.jpg')\n",
    "\n",
    "# Convert the image to RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# We resize the image to the input width and height of the first layer of the network.\n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da8616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17479272ff0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the images\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db10a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Display the resized images\n",
    "plt.subplot(122)\n",
    "plt.title('Resized Image')\n",
    "plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be1224",
   "metadata": {},
   "source": [
    "*Setting the Non-Maximal Suppression Threshold and the intersection Over Union Threshold*\n",
    "\n",
    "YOLO uses **Non-Maximal Suppression (NMS)** to only keep the best bounding box. The first step in NMS is to remove all the predicted bounding boxes that have a detection probability that is less than a given NMS threshold.  In the code below, we set this NMS threshold to `0.6`. This means that all predicted bounding boxes that have a detection probability less than 0.6 will be removed.\n",
    "\n",
    "After removing all the predicted bounding boxes that have a low detection probability, the second step in NMS, is to select the bounding boxes with the highest detection probability and eliminate all the bounding boxes whose **Intersection Over Union (IOU)** value does not match  a given IOU threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9382eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the NMS threshold\n",
    "nms_thresh = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the IOU threshold\n",
    "iou_thresh = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05830f57",
   "metadata": {},
   "source": [
    "*Object Detection*\n",
    "\n",
    "Once the image has been loaded and resized, and you have chosen your parameters for `nms_thresh` and `iou_thresh`, we can use the YOLO algorithm to detect objects in the image. We detect the objects using the `detect_objects(m, resized_image, iou_thresh, nms_thresh)`function from the `utils` module. This function takes in the model `m` returned by *Darknet*, the resized image, and the NMS and IOU thresholds, and returns the bounding boxes of the objects found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default figure size\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread('Melanoma.jpg')\n",
    "\n",
    "# Convert the image to RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# We resize the image to the input width and height of the first layer of the network.\n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Set the IOU threshold. Default value is 0.4\n",
    "iou_thresh = 0.4\n",
    "\n",
    "# Set the NMS threshold. Default value is 0.6\n",
    "nms_thresh = 0.6\n",
    "\n",
    "# Detect objects in the image\n",
    "boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n",
    "\n",
    "# Print the objects found and the confidence level\n",
    "print_objects(boxes, class_names)\n",
    "\n",
    "#Plot the image with bounding boxes and corresponding object class labels\n",
    "plot_boxes(original_image, boxes, class_names, plot_labels = True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
